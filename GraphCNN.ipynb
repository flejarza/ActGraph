{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "\n",
    "import math\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import init\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.modules.module import Module\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from numpy import linalg as LA\n",
    "import networkx as nx\n",
    "\n",
    "\n",
    "import pickle\n",
    "import argparse\n",
    "from torch import autograd\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pygcn import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pygcn.layers.GraphConvolution"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ACT_GRAPH(nn.Module):\n",
    "    def __init__(self, nfeat, nhid, nout, dropout):\n",
    "        super(ACT_GRAPH, self).__init__()\n",
    "\n",
    "        self.gc1 = GraphConvolution(nfeat, nhid)\n",
    "        self.gc2 = GraphConvolution(nhid, nout)\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        #This is a gray area, depending on the data representaion \n",
    "        \n",
    "        \n",
    "    def forward(self, V, A):\n",
    "        #Process the graph\n",
    "        x = F.relu(self.gc1(V, A))\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = self.gc2(x, A)\n",
    "        #Now process the info inside it \n",
    "        \n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "#Model specific parameters\n",
    "parser.add_argument('--nfeat', type=int, default=2)\n",
    "parser.add_argument('--nout', type=int, default=2)\n",
    "parser.add_argument('--nhid', type=int, default=5)\n",
    "parser.add_argument('--dropout', type=float, default=0.2)\n",
    "\n",
    "\n",
    "#Training specifc parameters\n",
    "parser.add_argument('--batch_size', type=int, default=128,\n",
    "                    help='minibatch size')\n",
    "parser.add_argument('--num_epochs', type=int, default=250,\n",
    "                    help='number of epochs')  \n",
    "parser.add_argument('--clip_grad', type=float, default=None,\n",
    "                    help='gadient clipping')        \n",
    "parser.add_argument('--lr', type=float, default=0.01,\n",
    "                    help='learning rate')\n",
    "parser.add_argument('--lr_sh_rate', type=int, default=150,\n",
    "                    help='number of steps to drop the lr')  \n",
    "parser.add_argument('--use_lrschd', action=\"store_true\", default=False,\n",
    "                    help='Use lr rate scheduler')\n",
    "parser.add_argument('--tag', default='tag',\n",
    "                    help='personal tag for the model ')\n",
    "                    \n",
    "args = parser.parse_args(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model and loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('*'*30)\n",
    "print(\"Training initiating....\")\n",
    "print(args)\n",
    "\n",
    "\n",
    "def graph_loss(V_pred,V_target):\n",
    "    return #What ever loss we thik it fits \n",
    "\n",
    "#Data prep     \n",
    "\n",
    "\n",
    "dset_train =# stump \n",
    "\n",
    "loader_train = DataLoader(\n",
    "        dset_train,\n",
    "        batch_size=1, #This is irrelative to the args batch size parameter\n",
    "        shuffle =True,\n",
    "        num_workers=0)\n",
    "\n",
    "\n",
    "dset_val = #stump\n",
    "\n",
    "loader_val = DataLoader(\n",
    "        dset_val,\n",
    "        batch_size=1, #This is irrelative to the args batch size parameter\n",
    "        shuffle =False,\n",
    "        num_workers=1)\n",
    "\n",
    "\n",
    "#Defining the model \n",
    "\n",
    "model = ACT_GRAPH(nfeat=args.nfeat, nhid=args.nhid, nout=args.nout, dropout=args.dropout).cuda()\n",
    "\n",
    "\n",
    "#Training settings \n",
    "\n",
    "optimizer = optim.SGD(model.parameters(),lr=args.lr)\n",
    "\n",
    "if args.use_lrschd:\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=args.lr_sh_rate, gamma=0.2)\n",
    "    \n",
    "\n",
    "\n",
    "checkpoint_dir = './checkpoint/'+args.tag+'/'\n",
    "\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)\n",
    "    \n",
    "with open(checkpoint_dir+'args.pkl', 'wb') as fp:\n",
    "    pickle.dump(args, fp)\n",
    "    \n",
    "\n",
    "\n",
    "print('Data and model loaded')\n",
    "print('Checkpoint dir:', checkpoint_dir)\n",
    "\n",
    "\n",
    "\n",
    "metrics = {'train_loss':[],  'val_loss':[]}\n",
    "constant_metrics = {'min_val_epoch':-1, 'min_val_loss':9999999999999999}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Valid functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training \n",
    "\n",
    "def train(epoch):\n",
    "    global metrics,loader_train\n",
    "    model.train()\n",
    "    loss_batch = 0 \n",
    "    batch_count = 0\n",
    "    is_fst_loss = True\n",
    "    loader_len = len(loader_train)\n",
    "    turn_point =int(loader_len/args.batch_size)*args.batch_size+ loader_len%args.batch_size -1\n",
    "\n",
    "\n",
    "    for cnt,batch in enumerate(loader_train): \n",
    "        batch_count+=1\n",
    "\n",
    "        #Get data\n",
    "        batch = [tensor.cuda() for tensor in batch]\n",
    "        V,A,Vout = batch\n",
    "\n",
    "\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        V_pred = model(V,A)\n",
    "                \n",
    "        V_tr = V.squeeze()\n",
    "        A_tr = A.squeeze()\n",
    "        V_pred = V_pred.squeeze()\n",
    "\n",
    "        if batch_count%args.batch_size !=0 and cnt != turn_point :\n",
    "            l = graph_loss(V_pred,V_tr)\n",
    "            if is_fst_loss :\n",
    "                loss = l\n",
    "                is_fst_loss = False\n",
    "            else:\n",
    "                loss += l\n",
    "\n",
    "        else:\n",
    "            loss = loss/args.batch_size\n",
    "            is_fst_loss = True\n",
    "            loss.backward()\n",
    "            \n",
    "            if args.clip_grad is not None:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(),args.clip_grad)\n",
    "\n",
    "\n",
    "            optimizer.step()\n",
    "            #Metrics\n",
    "            loss_batch += loss.item()\n",
    "            print('TRAIN:','\\t Epoch:', epoch,'\\t Loss:',loss_batch/batch_count)\n",
    "            \n",
    "    metrics['train_loss'].append(loss_batch/batch_count)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def vald(epoch):\n",
    "    global metrics,loader_val,constant_metrics\n",
    "    model.eval()\n",
    "    loss_batch = 0 \n",
    "    batch_count = 0\n",
    "    is_fst_loss = True\n",
    "    loader_len = len(loader_val)\n",
    "    turn_point =int(loader_len/args.batch_size)*args.batch_size+ loader_len%args.batch_size -1\n",
    "    \n",
    "    for cnt,batch in enumerate(loader_val): \n",
    "        batch_count+=1\n",
    "\n",
    "        #Get data\n",
    "        batch = [tensor.cuda() for tensor in batch]\n",
    "        V,A,Vout = batch\n",
    "\n",
    "    \n",
    "        V_obs_tmp =V_obs.permute(0,3,1,2)\n",
    "\n",
    "        V_pred = model(V_obs_tmp,A_obs.squeeze())\n",
    "        \n",
    "        \n",
    "        V_tr = A.squeeze()\n",
    "        A_tr = A.squeeze()\n",
    "        V_pred = V_pred.squeeze()\n",
    "\n",
    "        if batch_count%args.batch_size !=0 and cnt != turn_point :\n",
    "            l = graph_loss(V_pred,V_tr)\n",
    "            if is_fst_loss :\n",
    "                loss = l\n",
    "                is_fst_loss = False\n",
    "            else:\n",
    "                loss += l\n",
    "\n",
    "        else:\n",
    "            loss = loss/args.batch_size\n",
    "            is_fst_loss = True\n",
    "            #Metrics\n",
    "            loss_batch += loss.item()\n",
    "            print('VALD:','\\t Epoch:', epoch,'\\t Loss:',loss_batch/batch_count)\n",
    "\n",
    "    metrics['val_loss'].append(loss_batch/batch_count)\n",
    "    \n",
    "    if  metrics['val_loss'][-1]< constant_metrics['min_val_loss']:\n",
    "        constant_metrics['min_val_loss'] =  metrics['val_loss'][-1]\n",
    "        constant_metrics['min_val_epoch'] = epoch\n",
    "        torch.save(model.state_dict(),checkpoint_dir+'val_best.pth')  # OK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training started ...')\n",
    "for epoch in range(args.num_epochs):\n",
    "    train(epoch)\n",
    "    vald(epoch)\n",
    "    if args.use_lrschd:\n",
    "        scheduler.step()\n",
    "\n",
    "\n",
    "    print('*'*30)\n",
    "    print('Epoch:',args.tag,\":\", epoch)\n",
    "    for k,v in metrics.items():\n",
    "        if len(v)>0:\n",
    "            print(k,v[-1])\n",
    "\n",
    "\n",
    "    print(constant_metrics)\n",
    "    print('*'*30)\n",
    "    \n",
    "    with open(checkpoint_dir+'metrics.pkl', 'wb') as fp:\n",
    "        pickle.dump(metrics, fp)\n",
    "    \n",
    "    with open(checkpoint_dir+'constant_metrics.pkl', 'wb') as fp:\n",
    "        pickle.dump(constant_metrics, fp)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
